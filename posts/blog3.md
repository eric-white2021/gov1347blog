## Blog Post 3 - Polling
### September 28th, 2020

In this week's blog, I wanted to explore how we can use polling data to predict the outcome of elections. FiveThirtyEight has developed a model to weigh polls based on the quality, size, and recency of the polls. They determine the quality of the polls based on how these polls have performed recently. The data available this week (which can be found on my Github) included data about the weight of the polls for the 2016 election, which was helpful in determining how to evaluate the polling data and use it to make conclusions.
This week, I first examined how polls performed for incumbent and non-incumbent party candidates in presidential elections from 1968 to 2016. I then explored the polling data for the 2016 election on a state-by-state basis to understand how closely the polls matched the actual results, and I found that they were close on almost every election. I also looked into the swing states of Arizona, Florida, Michigan, Ohio, Pennsylvania, & Wisconsin to see how each candidate performed against the polls in these crucial states.
Finally, I explored some similarities and differences between [FiveThirtyEight's](https://fivethirtyeight.com/features/how-fivethirtyeights-2020-presidential-forecast-works-and-whats-different-because-of-covid-19/) election model and [The Economist's](https://projects.economist.com/us-2020-forecast/president/how-this-works) election model


## Key takeaways
1. Polls are better at predicting the vote share for the incumbent than for the challenger. Polls done closer to the election seem to be marginally more informative than ones done farther away.
2. In general, the polls in 2016 were fairly accurate. In the 6 swing states I explored, there were some discrepancies and the discrepancies tended to benefit Trump. On average, he outperformed polls by **6 percentage points** in these states, while Clinton only outperformed by **1.6 percentage points**.
3. The FiveThirtyEight and Economist models are relatively similar in terms of the statistical background. By and large, the FiveThirtyEight model seems a bit more robust and I tend to prefer it, but it also could be the case that they are incorporating too many unncessary variables that could make the result less meaningful.

## Past National Poll Performance
I examined the polling accuracy for polls conducted two weeks or less before election day for both the incumbent and non-incumbent parties, as well as the polling accuracy for polls conducted more than two weeks from the election.
It was interesting to see the degree to which the error bands around the best fit line were much narrower for the incumbent party than the non-incumbent party. I would have expected a relatively similar trend because there tends to only be two candidates, so naturally one candidate performing well would lead to the other candidate underperforming, but there have been cases of third parties influencing elections (1992, 1996 & 2000 being the most notable examples in recent history). Additionally, some respondents to polls do not provide a binary answer to the poll, so it could be the case that because there tends to be more uncertainty around the non-incumbent party's nominee that we would then see uncertainty in the predictive power of polling about this nominee, which seems to be the case from this data set.
One thing that stuck out to me in the explanation of FiveThirtyEight's model is that they weigh polls more heavily closer to election day (The Economist does as well but not to the same extent, it appears). I looked at the predictive power of the polling data two weeks or closer to the election and the polling data from two weeks or more until the election. For both the incumbent and non-incumbent parties, the general trend was the same, but there was an increase in the uncertainty, though seemingly not an entirely meaningful one. 

![](https://github.com/eric-white2021/gov1347blog/blob/gh-pages/National_polling_vote_share.png?raw=true)

## 2016 Polling
The accuracy of the 2016 polls was a controversial topic, and although they performed well in general, they did not perform well in crucial swing states that gave Trump the election. I looked at the national trend first, where the relationship between the polls and the actual result was fairly close. This may be because I used the weights that were part of the data set to weigh the polls properly, though these weights could have been determined retroactively, thus negative the importance of the result. I took the weight on each poll as a fraction of the total weight placed on all of the polls and multiplied it by the share of the vote won by Trump and Clinton in each of the polls, then combined these polls together for each state, thus getting a weighted average of the polls for the 2016 election cycle.

The states that I explored were Arizona, Florida, Michigan, Ohio, Pennsylvania, & Wisconsin, which are all important swing states and this cycle and are all (with the possible exception of Arizona) consistently important swing states. In Michigan, Ohio, Pennsylvania, and Wisconsin, Trump outperformed the weighted polling average by over 6 percentage points in each state, with the polls underrating Trump by as much as 7.26 percentage points in Ohio, a frequent swing state that went to Obama twice and then was won comfortably by Trump. This will be interesting to monitor for this election and may require the weights of certain polls to be adjusted.

![](https://github.com/eric-white2021/gov1347blog/blob/gh-pages/2016polling.png?raw=true)
